{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22706b86-d366-4390-bedf-d0833435bc4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.0 Loading a Training Data Sample into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deed77f-3f61-45bb-abe0-ef8ea6e09fe1",
   "metadata": {},
   "source": [
    "In this example, we'll walk through a heuristics-based automated feature selection process for an autoencoder based digital fingerprinting workflow. The selection process can be a compute and time intensive process on large datasets. Consequently, it's recommended that you select datasets that meet the following criteria for an initial exploratory analysis.\n",
    "\n",
    "1. Tractable Size: Chose a sample of your dataset that can easily fit into GPU and host memory. As an example, limiting your sample size to ~10,000 rows will result in a runtime of approximately 2 minutes. We recommend experimenting with a few different dataset sizes to find one that works best for you. \n",
    "2. Capturing Data Variance: Chose a sample of data that sufficiently captures the general behavior you're trying to model. For example, if you have a lot of categorical columns in your dataset, try and ensure your sample contains most, if not all, values your categorical variables can take. Also aim to capture as much variance (diversity) in your data as possible to get the best results out of this process. \n",
    "3. Don't attempt to optimize on too many features: While the automated process is capable of handling an arbitrary number of features (within system limits), try to optimize your process on only those features that may have cyber-specific relevance to the behavior you're trying to capture. Including a very large number of features to this analysis will noticably increase runtime and could also lead to suboptimal feature seleciton. \n",
    "\n",
    "With that being said, let's explore what a sample dataset could look like. Please note that your data should be **JSON formatted** in JSON files for this to work. If you're reading instead from something like `parquet` or CSV, we suggest reading it into a DataFrame with `pandas` and then turning that into a dictionary using `df.to_dict(orient='records')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce603a32-99f7-4e62-beb0-8013948fa8e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded data contains 3239 rows. An exampled data entry looks like: \n",
      "\n",
      "{'Level': 4,\n",
      " 'callerIpAddress': '13.113.40.157',\n",
      " 'category': 'NonInteractiveUserSignInLogs',\n",
      " 'correlationId': '36764e36-d379-45f4-914a-96a69bd59ae5',\n",
      " 'durationMs': 0,\n",
      " 'identity': 'Attack Target',\n",
      " 'location': 'XR',\n",
      " 'operationName': 'Sign-in activity',\n",
      " 'operationVersion': '1.0',\n",
      " 'properties': {'appDisplayName': 'Articulate 360',\n",
      "                'appId': '9c5b7fe3-0ad2-4ea6-94e5-9e0001f367e3',\n",
      "                'appServicePrincipalId': None,\n",
      "                'appliedConditionalAccessPolicies': [],\n",
      "                'authenticationContextClassReferences': [],\n",
      "                'authenticationDetails': [],\n",
      "                'authenticationProcessingDetails': [],\n",
      "                'authenticationProtocol': 'none',\n",
      "                'authenticationRequirement': 'singleFactorAuthentication',\n",
      "                'authenticationRequirementPolicies': [],\n",
      "                'autonomousSystemNumber': 34974,\n",
      "                'clientAppUsed': 'Mobile Apps and Desktop clients',\n",
      "                'clientCredentialType': 'none',\n",
      "                'conditionalAccessStatus': 'success',\n",
      "                'correlationId': '2a7afd02-0af7-4919-b42e-e96c358011e4',\n",
      "                'createdDateTime': '2022-08-05T18:07:31.755011Z',\n",
      "                'crossTenantAccessType': 'none',\n",
      "                'deviceDetail': {'browser': 'Edge 99.14477',\n",
      "                                 'deviceId': 'a44625dc-6f81-449a-9799-8005f7209b42',\n",
      "                                 'displayName': 'ATTACKTARGET-LT',\n",
      "                                 'operatingSystem': 'Windows 10',\n",
      "                                 'trustType': 'Azure AD registered'},\n",
      "                'flaggedForReview': False,\n",
      "                'homeTenantId': 'd3e5a967-5657-4a42-afcc-6106b6c3c299',\n",
      "                'id': '528a72ae-c612-474f-a22e-2f69e7ca7700',\n",
      "                'incomingTokenType': 'primaryRefreshToken',\n",
      "                'ipAddress': '13.113.40.157',\n",
      "                'isInteractive': False,\n",
      "                'isTenantRestricted': False,\n",
      "                'location': {'city': 'Smithfort',\n",
      "                             'countryOrRegion': 'XR',\n",
      "                             'geoCoordinates': {'latitude': 3.7564095,\n",
      "                                                'longitude': -121.574606},\n",
      "                             'state': 'Smithfort'},\n",
      "                'networkLocationDetails': [],\n",
      "                'originalRequestId': '1adc0f22-2e25-4831-a411-315dcf995f07',\n",
      "                'privateLinkDetails': {},\n",
      "                'processingTimeInMilliseconds': 313,\n",
      "                'resourceDisplayName': 'Articulate 360 Online',\n",
      "                'resourceId': '436260d5-3791-4aea-a4da-aa698ec35f2f',\n",
      "                'resourceServicePrincipalId': '94b9f3ba-5f4b-4edc-99dc-a420b76960f9',\n",
      "                'resourceTenantId': 'd3e5a967-5657-4a42-afcc-6106b6c3c299',\n",
      "                'riskDetail': 'none',\n",
      "                'riskEventTypes': [],\n",
      "                'riskEventTypes_v2': [],\n",
      "                'riskLevelAggregated': 'none',\n",
      "                'riskLevelDuringSignIn': 'none',\n",
      "                'riskState': 'none',\n",
      "                'rngcStatus': 0,\n",
      "                'servicePrincipalId': '',\n",
      "                'ssoExtensionVersion': '',\n",
      "                'status': {'errorCode': 0},\n",
      "                'tokenIssuerName': '',\n",
      "                'tokenIssuerType': 'AzureAD',\n",
      "                'uniqueTokenIdentifier': 'KZ4AsNDKZSYNt3xpOb8Wvlfn7uWHiUcyz0JxdB4i6K7h0Br8',\n",
      "                'userAgent': 'Mozilla/5.0 (iPad; CPU iPad OS 9_3_6 like Mac OS '\n",
      "                             'X) AppleWebKit/536.1 (KHTML, like Gecko) '\n",
      "                             'CriOS/42.0.865.0 Mobile/77O124 Safari/536.1 '\n",
      "                             'Edge/99.14477',\n",
      "                'userDisplayName': 'Attack Target',\n",
      "                'userId': 'd735e84b-dcca-404d-9f7d-700f360f41a6',\n",
      "                'userPrincipalName': 'attacktarget@domain.com',\n",
      "                'userType': 'Member'},\n",
      " 'resourceId': '/tenants/d3e5a967-5657-4a42-afcc-6106b6c3c299/providers/Microsoft.aadiam',\n",
      " 'resultDescription': None,\n",
      " 'resultSignature': 'None',\n",
      " 'resultType': '0',\n",
      " 'tenantId': 'd3e5a967-5657-4a42-afcc-6106b6c3c299',\n",
      " 'time': '2022-08-05T18:07:31.442011Z'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "def concatenate_json_lists(directory_path):\n",
    "    \"\"\"\n",
    "    Reads all JSON files in the specified directory, assuming each JSON file contains a list,\n",
    "    and concatenates these lists into a single list.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): The path to the directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - list: A concatenated list containing all elements from the lists in the JSON files.\n",
    "    \"\"\"\n",
    "    concatenated_list = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    if isinstance(data, list):\n",
    "                        concatenated_list.extend(data)\n",
    "                    else:\n",
    "                        print(f\"File {filename} does not contain a list.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing file {filename}: {e}\")\n",
    "    return concatenated_list\n",
    "\n",
    "concatenated_data = concatenate_json_lists(\"/workspace/examples/data/dfp/azure-training-data/\") #Change the path to your data directory here. \n",
    "\n",
    "print(f\"The loaded data contains {len(concatenated_data)} rows. An exampled data entry looks like: \\n\")\n",
    "pprint(concatenated_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab34bc-3cf9-4ce4-96e3-415dad563581",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 Performing Automated Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcf1f0-15bc-4080-906c-c3e5cdccb85c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Known Limitations and Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fd5c2-1d6b-4133-bb6d-761a648532d8",
   "metadata": {},
   "source": [
    "Once we have the data oriented as a list of dictionary objects (JSON), we're ready to run the automated feature selection process. \n",
    "\n",
    "__The tool is domain agnostic__, which means that it doesn't select features that work well for your specific cyber workflow. Instead, it selects features that work well for an autoencoder model from a statistical perspective. This necessarily introduces a few considerations we recommend taking into account.\n",
    "\n",
    "1. **Consult with cyber experts**: We recommend analyizing the output of the feature selection process with cybersecurity experts in your domain area to verify if they believe the features contain enough 'signal' from a use-case persepctive. Domain experts can also help give you an idea of how they would solve the problem, which can inform your feature selection. \n",
    "2. **Limit the possibility of overfitting**: Avoid selecting feature outputs from the model that could lead to undesrible properties like model overfitting. For example, avoid using features such as individually identifiable IP addresses, usernames, or MAC addresses outside of the actual user attribute in the DFP pipeline. \n",
    "3. **Derived features**: This tool does not suggest or create any derived features. Such features are often helpful, if not critical, in the successful functioning of advanced ML workflows. Consider adding some derived features that could be helpful in capturing the behavior of interest into the data. This can often be done in collaboration with cyber domain experts who can help inform how they would solve the problem, which can be tranlated into derived features. \n",
    "4. **Alternate encodings**: By default, any non-numeric features are one-hot encoded as categorical variables by the tool. Often, there may be better ways of representing the data that lend themselves well to your use case. For example, only encoding variables of interest, embedding text instead of one-hot encoding them, etc. We recommend exploring alternatives once you've established a baseline\n",
    "\n",
    "Refer to the notebook demo [here](https://github.com/nv-morpheus/Morpheus/blob/branch-24.06/models/training-tuning-scripts/dfp-models/dfp-feature-selection-demo.ipynb) for a more detailed analysis of datasets and ideas on derived features. The automated method here is intended to be used purely as a starting point in your development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2d69e-8cc1-407f-875f-f575be15c946",
   "metadata": {},
   "source": [
    "## 2.2. Running Automated Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b498cf3-8f94-436d-861a-fd8a17186a5e",
   "metadata": {},
   "source": [
    "We can run automated feature selection by using Morpheus' `AutoencoderFeatureSelector` class and configuring some parameters. Let's see how. The AutoencoderFeatureSelector class provides the following configurable parameters. \n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "    A class to select features using an autoencoder, handling categorical and numerical data.\n",
    "    Supports handling ambiguities in data types and ensures selected feature count constraints.\n",
    "\n",
    "    Attributes:\n",
    "        input_json (dict): List of dictionary objects to normalize into a dataframe\n",
    "        id_column (str) : Column name that contains ID for morpheus AE pipeline. Default None.\n",
    "        timestamp_column (str) : Column name that contains the\n",
    "            timestamp for morpheus AE pipeline. Default None.\n",
    "        encoding_dim (int): Dimension of the encoding layer, defaults to half of input\n",
    "            dimensions if not set.\n",
    "        batch_size (int): Batch size for training the autoencoder.\n",
    "        variance_threshold (float) : Minimum variance a column must contain\n",
    "            to remain in consideration. Default 0.\n",
    "        null_threshold (float): Maximum proportion of null values a column can contain. Default 0.3.\n",
    "        cardinality_threshold_high (float): Maximum proportion\n",
    "            of cardinality to length of data allowable. Default 0.99.\n",
    "        cardinality_threshold_low_n (int): Minimum cardinalty for a feature to be considered numerical\n",
    "            during type infernce. Default 10.\n",
    "        categorical_features (list[str]): List of features in the data to be considered categorical. Default [].\n",
    "        numeric_features (list[str]): List of features in the data to be considered numeric. Default [].\n",
    "        ablation_epochs (int): Number of epochs to train the autoencoder.\n",
    "        device (str): Device to run the model on, defaults to 'cuda' if available.\n",
    "        \n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b071d622-cbcc-429d-bc49-2a7fd1b6fb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "sys.path.insert(0, '/workspace/morpheus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd671cda-73ca-49dd-8cf0-02aa63adfe1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.dfencoder.ae_feature_selector import AutoencoderFeatureSelector \n",
    "\n",
    "selector = AutoencoderFeatureSelector(\n",
    "    input_json = concatenated_data, \n",
    "    id_column = 'identity', #This is the entity you want to 'fingerprint'\n",
    "    timestamp_column = 'time', #This is your log timestamp\n",
    "    variance_threshold=0.1, #Removes cols. with variance lower than this\n",
    "    null_threshold=0.3, #Removes cols will null proportion great than this\n",
    "    cardinality_threshold_high=0.9, #Remove columns with high cardinality\n",
    "    cardinality_threshold_low_n=10, #Cols. with this cardinality with considered categorica\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03995194-23b1-453a-8714-bfc6a3cb37e3",
   "metadata": {},
   "source": [
    "Once we've instantiated the class, we can run the feature selection as a one-line command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe23de-e3c0-43f7-83de-771de6919619",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Categorical or numeric features not provided. Performing type inference which could be inaccurate.\n",
      "Found sparse arrays when one-hot encoding. Consider using fewer categorical variables.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc78c7b9d34cd5baebdc4c52960447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AE Ablation Study:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n",
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "Not going to perform early-stopping. self.patience(=-1) is provided for early-stopping but validation is not enabled. Please set `run_validation` to True and provide a `validation_dataset` to enable early-stopping.\n"
     ]
    }
   ],
   "source": [
    "schemas = selector.select_features(\n",
    "    k_min = 5, #Select at minimum 5 features\n",
    "    k_max = 20, #Select at most 20 features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ba70-a130-4424-abf8-7bfc1f48d16b",
   "metadata": {},
   "source": [
    "The output of the selection process is a report of what operations were performed on the data as well as the most important features the class thinks is important when building an autoencoder. The results are ranked based on the output of a generic __ablation study__ wherein a very basic autoencoder is trained on various combinations of features to identify which features help it learn the 'most'. \n",
    "\n",
    "Please not here, too, that all of these combinations are purely heuristics. We recommend augmenting or verifying them with more detailed analyses and with domain experts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ba680-1606-4609-8f73-7df1ace4e3d8",
   "metadata": {},
   "source": [
    "## 2.3 Extracting Schema Objects for Morpheus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f0091-1e15-4789-86fb-683842bbada6",
   "metadata": {},
   "source": [
    "The feature selector also dynamically builds JSON schema representations of the selected and raw features which you can use in a Morpheus pipeline using the `JSONSchemaBuilder` class for `DataFrameInputSchemas`. Let's examine what recommended initial schema for the provided data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6941ff6c-6ac9-43b3-be88-24c184fd6756",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JSON_COLUMNS': ['properties'],\n",
      " 'SCHEMA_COLUMNS': [{'data_column': 'category',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'operationName',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'tenantId',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'operationVersion',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'callerIpAddress',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'resultSignature',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'durationMs',\n",
      "                     'dtype': 'int',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'resourceId',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'resultType',\n",
      "                     'dtype': 'int',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'properties.location.geoCoordinates.latitude',\n",
      "                     'dtype': 'float',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'properties.location.geoCoordinates.longitude',\n",
      "                     'dtype': 'float',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'properties.processingTimeInMilliseconds',\n",
      "                     'dtype': 'int',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'identity',\n",
      "                     'dtype': 'string',\n",
      "                     'type': 'ColumnInfo'},\n",
      "                    {'data_column': 'time',\n",
      "                     'dtype': 'datetime',\n",
      "                     'name': 'time',\n",
      "                     'type': 'DateTimeColumn'}]}\n"
     ]
    }
   ],
   "source": [
    "pprint(schemas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fb7d2c-77d2-477e-9b15-2dc2be053f4a",
   "metadata": {},
   "source": [
    "These schemas can also be saved to JSON files by providing the `raw_schema_path` and `preprocess_schema_path` path arugments to the `select_features` function. \n",
    "\n",
    "Please refer to the __Morpheus/examples/digital_fingerprinting/production/morpheus/notebooks/json_schema_builder.ipynb__ notebook for a demo on using JSON schema files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
