{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - Azure Integrated Training\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs both training and inference on Azure-AD logs. The goal is to train an autoencoder PyTorch model to recogize the patterns of users in the sample data. The model will then be used by another fork (inference) in the pipeline to generate anomaly scores for each individual log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP integrated training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import typing\n",
    "import cudf\n",
    "from datetime import datetime\n",
    "\n",
    "from dfp.modules import dfp_deployment  # noqa: F401\n",
    "from dfp.utils.config_generator import ConfigGenerator\n",
    "from dfp.utils.config_generator import generate_ae_config\n",
    "from dfp.utils.dfp_arg_parser import DFPArgParser\n",
    "from dfp.utils.schema_utils import Schema\n",
    "from dfp.utils.schema_utils import SchemaBuilder\n",
    "\n",
    "from morpheus.config import Config\n",
    "from morpheus.pipeline.pipeline import Pipeline\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.stages.general.multiport_modules_stage import MultiPortModulesStage\n",
    "from morpheus.stages.input.control_message_file_source_stage import ControlMessageFileSourceStage\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The pipeline's functionality can be significantly altered by the following options, which are utilized across the entire pipeline. However, module-specific options also exist. The matching Python script for this notebook, `dfp_integrated_training_batch_pipeline.py`, configures these options through command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name                   | Type                                       | Description                                                                                                                                                                                                                                                                            | Default Value |\n",
    "|------------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `source`            | One of `[\"duo\", \"azure\"]`           | Indicates what type of logs are going to be used in the workload.                                                                                                                                                                                                            | -             |\n",
    "| `train_users`    | One of `[\"all\", \"generic\", \"individual\"]` | Indicates whether or not to train per user or a generic model for all users. Selecting none runs the inference pipeline.                                                                                                                                                                    | -             |\n",
    "| `skip_user`        | List of strings                                       | User IDs to skip. Mutually exclusive with `only_user`.                                                                                                                                                                                                                                               | -             |\n",
    "| `only_user`        | List of strings                                       | Only users specified by this option will be included. Mutually exclusive with `skip_user`.                                                                                                                                                                                                 | -             |\n",
    "| `start_time`       | `str`                                         | The start of the time window, if undefined start_date will be `now()-duration`.                                                                                                                                                                                                  | -             |\n",
    "| `duration`         | `str`                                         | The training duration to run starting from `start_time`.                                                                                                                                                                                                                                              | -             |\n",
    "| `cache_dir`        | `str`                                         | The location to cache data such as S3 downloads and pre-processed data.                                                                                                                                                                                                                    | -             |\n",
    "| `log_level`        | `str`                                         | Specify the logging level to use.                                                                                                                                                                                                                                                                    | `info`        |\n",
    "| `sample_rate_s`    | `int`                                         | Minimum time step, in milliseconds, between object logs.                                                                                                                                                                                                                                           | `0`        |\n",
    "| `silence_monitors`    | `bool`                                         | Controls whether monitors will be verbose logs.                                                                                                                                                                                                                                           | `False`        |\n",
    "| `tracking_uri`     | `str`                                         | The MLflow tracking URI to connect to the tracking backend.                                                                                                                                                                                                                                    | -             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "source = \"azure\"\n",
    "\n",
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.strptime(\"2022-08-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# Duration\n",
    "duration = \"60d\"\n",
    "\n",
    "# Smaple rate secs\n",
    "sample_rate_s = 0\n",
    "\n",
    "# MLFLow tracking uri\n",
    "tracking_uri = \"http://localhost:8000\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_user: typing.List[str] = []\n",
    "\n",
    "# Only users\n",
    "only_user: typing.List[str] = []\n",
    "\n",
    "# Setting Log level\n",
    "log_level = logging.WARN\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"/workspace/.cache/dfp\"\n",
    "\n",
    "# Silence monitors\n",
    "silence_monitors = True\n",
    "\n",
    "# Control messages as input files\n",
    "load_train_only_input_files = [\n",
    "    \"./resource/azure_payload_lt.json\"\n",
    "]\n",
    "load_train_inference_input_files = [\n",
    "    \"./resource/azure_payload_lti.json\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f80a19",
   "metadata": {},
   "source": [
    "### Arguments Parser\n",
    "\n",
    "The [DFPArgParser](../../../production/morpheus/dfp/utils/dfp_arg_parser.py) class is used for parsing and storing arguments used in a  pipeline for training, generating models and inference. It has several properties and methods to transform, store and access the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_arg_parser = DFPArgParser(\n",
    "    skip_user,\n",
    "    only_user,\n",
    "    start_time,\n",
    "    log_level,\n",
    "    cache_dir,\n",
    "    sample_rate_s,\n",
    "    duration,\n",
    "    source,\n",
    "    tracking_uri,\n",
    "    silence_monitors,\n",
    "    train_users\n",
    ")\n",
    "\n",
    "# Initalize parser\n",
    "dfp_arg_parser.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global config object for the pipeline\n",
    "config: Config = generate_ae_config(\n",
    "    source,\n",
    "    userid_column_name=\"username\",\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    use_cpp=True,\n",
    ")\n",
    "\n",
    "# Construct the dataframe Schema which is used to normalize incoming duo logs\n",
    "schema_builder = SchemaBuilder(config, source)\n",
    "schema: Schema = schema_builder.build_schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb42888",
   "metadata": {},
   "source": [
    "### DFP Deployment Module Configuration\n",
    "This module sets up modular Digital Fingerprinting intergated training pipeline instance.\n",
    "\n",
    "### Configurable Parameters\n",
    "\n",
    "| Parameter           | Type | Description                               | Example Value | Default Value |\n",
    "|---------------------|------|-------------------------------------------|---------------|---------------|\n",
    "| `inference_options` | dict | Options for the inference pipeline module | See Below     | `[Required]`  |\n",
    "| `training_options`  | dict | Options for the training pipeline module  | See Below     | `[Required]`  |\n",
    "\n",
    "### Training Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|---------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`           |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`    |\n",
    "| `dfencoder_options`          | dict | Options for configuring the data frame encoder | See Below            | `-`           |\n",
    "| `mlflow_writer_options`      | dict | Options for the MLflow model writer            | See Below            | `-`           |\n",
    "| `preprocessing_options`      | dict | Options for preprocessing the data             | See Below            | `-`           |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`           |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column used in the data  | \"my_timestamp\"       | `timestamp`   |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`           |\n",
    "\n",
    "### Inference Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value  |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|----------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`            |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`     |\n",
    "| `detection_criteria`         | dict | Criteria for filtering detections              | See Below            | `-`            |\n",
    "| `fallback_username`          | str  | User ID to use if user ID not found            | \"generic_user\"       | `generic_user` |\n",
    "| `inference_options`          | dict | Options for the inference module               | See Below            | `-`            |\n",
    "| `model_name_formatter`       | str  | Format string for the model name               | \"model_{timestamp}\"  | `[Required]`   |\n",
    "| `num_output_ports`           | int  | Number of output ports for the module          | 3                    | `-`            |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column in the input data | \"timestamp\"          | `timestamp`    |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`            |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`            |\n",
    "| `write_to_file_options`      | dict | Options for writing the detections to a file   | See Below            | `-`            |\n",
    "\n",
    "### `batching_options`\n",
    "\n",
    "| Key                      | Type            | Description                         | Example Value                               | Default Value              |\n",
    "|--------------------------|-----------------|-------------------------------------|---------------------------------------------|----------------------------|\n",
    "| `end_time`               | datetime/string | Endtime of the time window          | \"2023-03-14T23:59:59\"                       | `None`                     |\n",
    "| `iso_date_regex_pattern` | string          | Regex pattern for ISO date matching | \"\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\" | `<iso_date_regex_pattern>` |\n",
    "| `parser_kwargs`          | dictionary      | Additional arguments for the parser | {}                                          | `{}`                       |\n",
    "| `period`                 | string          | Time period for grouping files      | \"1d\"                                        | `D`                        |\n",
    "| `sampling_rate_s`        | integer         | Sampling rate in seconds            | 60                                          | `60`                       |\n",
    "| `start_time`             | datetime/string | Start time of the time window       | \"2023-03-01T00:00:00\"                       | `None`                     |\n",
    "\n",
    "### `dfencoder_options`\n",
    "\n",
    "| Parameter         | Type  | Description                            | Example Value                                                                                                                                                                                                                                                 | Default Value |\n",
    "|-------------------|-------|----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `feature_columns` | list  | List of feature columns to train on    | [\"column1\", \"column2\", \"column3\"]                                                                                                                                                                                                                             | `-`           |\n",
    "| `epochs`          | int   | Number of epochs to train for          | 50                                                                                                                                                                                                                                                            | `-`           |\n",
    "| `model_kwargs`    | dict  | Keyword arguments to pass to the model | {\"encoder_layers\": [64, 32], \"decoder_layers\": [32, 64], \"activation\": \"relu\", \"swap_p\": 0.1, \"lr\": 0.001, \"lr_decay\": 0.9, \"batch_size\": 32, \"verbose\": 1, \"optimizer\": \"adam\", \"scalar\": \"min_max\", \"min_cats\": 10, \"progress_bar\": false, \"device\": \"cpu\"} | `-`           |\n",
    "| `validation_size` | float | Size of the validation set             | 0.1                                                                                                                                                                                                                                                           | `-`           |\n",
    "\n",
    "### `monitor_options`\n",
    "\n",
    "| Key                          | Type    | Description                                                | Example Value | Default Value |\n",
    "| ----------------------------|---------|------------------------------------------------------------|---------------|---------------|\n",
    "| `description`               | string  | Name to show for this Monitor Stage in the console window  | \"Progress\"    | `Progress`    |\n",
    "| `silence_monitors`          | bool    | Silence the monitors on the console                        | See Below     | `None`        |\n",
    "| `smoothing`                 | float   | Smoothing parameter to determine how much the throughput should be averaged | 0.01 | `0.05` |\n",
    "| `unit`                      | string  | Units to show in the rate value                             | \"messages\"    | `messages`    |\n",
    "| `delayed_start`             | bool    | When delayed_start is enabled, the progress bar will not be shown until the first message is received. Otherwise, the progress bar is shown on pipeline startup and will begin timing immediately. In large pipelines, this option may be desired to give a more accurate timing. | True  | `False`   |\n",
    "| `determine_count_fn_schema` | string  | Custom function for determining the count in a message      | \"Progress\"    | `Progress`    |\n",
    "| `log_level`                 | string  | Enable this stage when the configured log level is at `log_level` or lower. | \"DEBUG\" | `INFO` |\n",
    "\n",
    "\n",
    "### `mlflow_writer_options`\n",
    "\n",
    "| Key                         | Type       | Description                       | Example Value                 | Default Value |\n",
    "|-----------------------------|------------|-----------------------------------|-------------------------------|---------------|\n",
    "| `conda_env`                 | string     | Conda environment for the model   | \"path/to/conda_env.yml\"       | `[Required]`  |\n",
    "| `databricks_permissions`    | dictionary | Permissions for the model         | See Below                     | `None`        |\n",
    "| `experiment_name_formatter` | string     | Formatter for the experiment name | \"experiment_name_{timestamp}\" | `[Required]`  |\n",
    "| `model_name_formatter`      | string     | Formatter for the model name      | \"model_name_{timestamp}\"      | `[Required]`  |\n",
    "| `timestamp_column_name`     | string     | Name of the timestamp column      | \"timestamp\"                   | `timestamp`   |\n",
    "\n",
    "### `stream_aggregation_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                                 | Example Value | Default Value |\n",
    "|-------------------------|--------|-------------------------------------------------------------|---------------|---------------|\n",
    "| `cache_mode`            | string | The user ID to use if the user ID is not found              | \"batch\"       | `batch`       |\n",
    "| `min_history`           | int    | Minimum history to trigger a new training event             | 1             | `1`           |\n",
    "| `max_history`           | int    | Maximum history to include in a new training event          | 0             | `0`           |\n",
    "| `timestamp_column_name` | string | Name of the column containing timestamps                    | \"timestamp\"   | `timestamp`   |\n",
    "| `aggregation_span`      | string | Lookback timespan for training data in a new training event | \"60d\"         | `60d`         |\n",
    "| `cache_to_disk`         | bool   | Whether or not to cache streaming data to disk              | false         | `false`       |\n",
    "| `cache_dir`             | string | Directory to use for caching streaming data                 | \"./.cache\"    | `./.cache`    |\n",
    "\n",
    "### `user_splitting_options`\n",
    "\n",
    "| Key                     | Type | Description                                          | Example Value               | Default Value  |\n",
    "|-------------------------|------|------------------------------------------------------|-----------------------------|----------------|\n",
    "| `fallback_username`     | str  | The user ID to use if the user ID is not found       | \"generic_user\"              | `generic_user` |\n",
    "| `include_generic`       | bool | Whether to include a generic user ID in the output   | false                       | `false`        |\n",
    "| `include_individual`    | bool | Whether to include individual user IDs in the output | true                        | `false`        |\n",
    "| `only_users`            | list | List of user IDs to include; others will be excluded | [\"user1\", \"user2\", \"user3\"] | `[]`           |\n",
    "| `skip_users`            | list | List of user IDs to exclude from the output          | [\"user4\", \"user5\"]          | `[]`           |\n",
    "| `timestamp_column_name` | str  | Name of the column containing timestamps             | \"timestamp\"                 | `timestamp`    |\n",
    "| `userid_column_name`    | str  | Name of the column containing user IDs               | \"username\"                  | `username`     |\n",
    "\n",
    "### `detection_criteria`\n",
    "\n",
    "| Key          | Type  | Description                              | Example Value | Default Value |\n",
    "|--------------|-------|------------------------------------------|---------------|---------------|\n",
    "| `threshold`  | float | Threshold for filtering detections       | 0.5           | `0.5`         |\n",
    "| `field_name` | str   | Name of the field to filter by threshold | \"score\"       | `probs`       |\n",
    "\n",
    "### `inference_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                          | Example Value           | Default Value |\n",
    "|-------------------------|--------|------------------------------------------------------|-------------------------|---------------|\n",
    "| `model_name_formatter`  | string | Formatter for model names                            | \"user_{username}_model\" | `[Required]`  |\n",
    "| `fallback_username`     | string | Fallback user to use if no model is found for a user | \"generic_user\"          | `generic_user`|\n",
    "| `timestamp_column_name` | string | Name of the timestamp column                         | \"timestamp\"             | `timestamp`   |\n",
    "\n",
    "### `write_to_file_options`\n",
    "\n",
    "| Key                 | Type      | Description                              | Example Value   | Default Value    |\n",
    "|---------------------|-----------|------------------------------------------|-----------------|------------------|\n",
    "| `filename`          | string    | Path to the output file                  | \"output.csv\"    | `None`           |\n",
    "| `file_type`         | string    | Type of file to write                    | \"CSV\"           | `AUTO`           |\n",
    "| `flush`             | bool      | If true, flush the file after each write | false           | `false`          |\n",
    "| `include_index_col` | bool      | If true, include the index column        | false           | `true`           |\n",
    "| `overwrite`         | bool      | If true, overwrite the file if it exists | true            | `false`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config helper is used to generate config parameters for the DFP module\n",
    "# This will populate to the minimum configuration parameters with intelligent default values\n",
    "config_generator = ConfigGenerator(config, dfp_arg_parser, schema)\n",
    "\n",
    "dfp_deployment_module_config = config_generator.get_module_conf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`ControlMessageFileSourceStage`)\n",
    "\n",
    "This pipeline read control message definations from one or more input files. This source stage will constructs control message and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any control message defination files to read into the pipeline |\n",
    "\n",
    "### DFP Deployment Module (`MultiPortModulesStage`)\n",
    "\n",
    "MultiPortModulesStage is used to load modules that returns more than one ouptut. DFP deployment module sets up modular Digital Fingerprinting Pipeline instance. and performs integrated training as shown in the below diagram. For more information on the options passed to this module is shown [here](../../../../../docs/source/modules/examples/digital_fingerprinting/dfp_deployment.md).\n",
    "\n",
    "\n",
    "## End-to-End Workflow Architecture\n",
    "\n",
    "![Integrated Training Pipeline](../images/dfp_integrated_training_file_pipeline.jpg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipeline():\n",
    "    # Create a pipeline object\n",
    "    pipeline = Pipeline(config)\n",
    "\n",
    "    # ControlMessage file source stage.\n",
    "    source_stage = pipeline.add_stage(ControlMessageFileSourceStage(config, filenames=input_files))\n",
    "\n",
    "    # DFP deployment (integrated training) module stage.\n",
    "    dfp_deployment_stage = pipeline.add_stage(\n",
    "        MultiPortModulesStage(config,\n",
    "                                dfp_deployment_module_config,\n",
    "                                input_port_name=\"input\",\n",
    "                                output_port_name_prefix=\"output\",\n",
    "                                num_output_ports=2))\n",
    "\n",
    "    # Connect stages with edges.\n",
    "    pipeline.add_edge(source_stage, dfp_deployment_stage)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b4b93cb",
   "metadata": {},
   "source": [
    "### Training\n",
    "To ensure a smooth deployment of the inference tasks to the pipeline, it is imperative to have at least one version of the trained model available on the MLflow server. Once the initial model training is complete, we can proceed with the publishing of the inference tasks to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0061d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_only_input_files\n",
    "\n",
    "pipeline = construct_pipeline()\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1472a0",
   "metadata": {},
   "source": [
    "### Training and Inference\n",
    "Now that we have a trained model available in MLflow, we can begin executing both training and inference tasks in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64339676",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_inference_input_files\n",
    "\n",
    "pipeline = construct_pipeline()\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a24caa6f",
   "metadata": {},
   "source": [
    "### Inference Results\n",
    "Pipeline writes the inference results to `dfp_detections_azure.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960f14ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>logcount</th>\n",
       "      <th>logcount_pred</th>\n",
       "      <th>logcount_loss</th>\n",
       "      <th>logcount_z_loss</th>\n",
       "      <th>locincrement</th>\n",
       "      <th>locincrement_pred</th>\n",
       "      <th>locincrement_loss</th>\n",
       "      <th>locincrement_z_loss</th>\n",
       "      <th>appincrement</th>\n",
       "      <th>...</th>\n",
       "      <th>statusfailureReason_pred</th>\n",
       "      <th>statusfailureReason_loss</th>\n",
       "      <th>statusfailureReason_z_loss</th>\n",
       "      <th>max_abs_z</th>\n",
       "      <th>mean_abs_z</th>\n",
       "      <th>z_loss_scaler_type</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model_version</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>6.294001</td>\n",
       "      <td>13.834214</td>\n",
       "      <td>10.950698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.423363</td>\n",
       "      <td>4.199121</td>\n",
       "      <td>2.667636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fresh auth token is needed. Have the user re-s...</td>\n",
       "      <td>1.926561</td>\n",
       "      <td>1.058883</td>\n",
       "      <td>10.950698</td>\n",
       "      <td>2.041574</td>\n",
       "      <td>z</td>\n",
       "      <td>acole@domain.com</td>\n",
       "      <td>2022-08-30T01:54:26.639083000Z</td>\n",
       "      <td>DFP-azure-acole@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>6.289782</td>\n",
       "      <td>15.659849</td>\n",
       "      <td>12.508736</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.424431</td>\n",
       "      <td>4.205431</td>\n",
       "      <td>2.672908</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fresh auth token is needed. Have the user re-s...</td>\n",
       "      <td>1.925017</td>\n",
       "      <td>1.089649</td>\n",
       "      <td>12.508736</td>\n",
       "      <td>2.214617</td>\n",
       "      <td>z</td>\n",
       "      <td>acole@domain.com</td>\n",
       "      <td>2022-08-30T01:54:26.639083000Z</td>\n",
       "      <td>DFP-azure-acole@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>6.287379</td>\n",
       "      <td>17.594990</td>\n",
       "      <td>14.160229</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.425592</td>\n",
       "      <td>4.212285</td>\n",
       "      <td>2.678633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fresh auth token is needed. Have the user re-s...</td>\n",
       "      <td>1.923433</td>\n",
       "      <td>1.121214</td>\n",
       "      <td>14.160229</td>\n",
       "      <td>2.397326</td>\n",
       "      <td>z</td>\n",
       "      <td>acole@domain.com</td>\n",
       "      <td>2022-08-30T01:54:26.639083000Z</td>\n",
       "      <td>DFP-azure-acole@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>6.285878</td>\n",
       "      <td>19.640940</td>\n",
       "      <td>15.906289</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.426562</td>\n",
       "      <td>4.218020</td>\n",
       "      <td>2.683426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fresh auth token is needed. Have the user re-s...</td>\n",
       "      <td>1.922159</td>\n",
       "      <td>1.146584</td>\n",
       "      <td>15.906289</td>\n",
       "      <td>2.589084</td>\n",
       "      <td>z</td>\n",
       "      <td>acole@domain.com</td>\n",
       "      <td>2022-08-30T01:54:26.639083000Z</td>\n",
       "      <td>DFP-azure-acole@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>6.283889</td>\n",
       "      <td>21.800470</td>\n",
       "      <td>17.749281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.427629</td>\n",
       "      <td>4.224331</td>\n",
       "      <td>2.688698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Fresh auth token is needed. Have the user re-s...</td>\n",
       "      <td>1.921002</td>\n",
       "      <td>1.169640</td>\n",
       "      <td>17.749281</td>\n",
       "      <td>2.791408</td>\n",
       "      <td>z</td>\n",
       "      <td>acole@domain.com</td>\n",
       "      <td>2022-08-30T01:54:26.639083000Z</td>\n",
       "      <td>DFP-azure-acole@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>152</td>\n",
       "      <td>16</td>\n",
       "      <td>7.445123</td>\n",
       "      <td>3.564877</td>\n",
       "      <td>1.963678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.780383</td>\n",
       "      <td>4.898170</td>\n",
       "      <td>1.954723</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>_other</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>332.180115</td>\n",
       "      <td>40.882931</td>\n",
       "      <td>z</td>\n",
       "      <td>attacktarget@domain.com</td>\n",
       "      <td>2022-08-31T23:54:50.435683000Z</td>\n",
       "      <td>DFP-azure-attacktarget@domain.com:2</td>\n",
       "      <td>2023-04-29T01:09:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>642</td>\n",
       "      <td>13</td>\n",
       "      <td>4.775130</td>\n",
       "      <td>3.045915</td>\n",
       "      <td>1.131403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.954752</td>\n",
       "      <td>0.525846</td>\n",
       "      <td>0.203420</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>1.680271</td>\n",
       "      <td>0.783752</td>\n",
       "      <td>13.603901</td>\n",
       "      <td>2.070845</td>\n",
       "      <td>z</td>\n",
       "      <td>attacktarget@domain.com</td>\n",
       "      <td>2022-08-31T23:45:08.870131000Z</td>\n",
       "      <td>DFP-azure-generic_user:543</td>\n",
       "      <td>2023-04-29T01:09:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>643</td>\n",
       "      <td>14</td>\n",
       "      <td>4.683823</td>\n",
       "      <td>3.907826</td>\n",
       "      <td>1.604047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.957831</td>\n",
       "      <td>0.529244</td>\n",
       "      <td>0.201969</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>1.630660</td>\n",
       "      <td>0.952875</td>\n",
       "      <td>16.555244</td>\n",
       "      <td>2.588548</td>\n",
       "      <td>z</td>\n",
       "      <td>attacktarget@domain.com</td>\n",
       "      <td>2022-08-31T23:48:48.732047000Z</td>\n",
       "      <td>DFP-azure-generic_user:543</td>\n",
       "      <td>2023-04-29T01:09:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>644</td>\n",
       "      <td>15</td>\n",
       "      <td>4.910330</td>\n",
       "      <td>4.583674</td>\n",
       "      <td>1.974660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.949175</td>\n",
       "      <td>0.519721</td>\n",
       "      <td>0.206035</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>1.599877</td>\n",
       "      <td>1.057811</td>\n",
       "      <td>19.275707</td>\n",
       "      <td>3.001436</td>\n",
       "      <td>z</td>\n",
       "      <td>attacktarget@domain.com</td>\n",
       "      <td>2022-08-31T23:51:17.852920000Z</td>\n",
       "      <td>DFP-azure-generic_user:543</td>\n",
       "      <td>2023-04-29T01:09:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>646</td>\n",
       "      <td>16</td>\n",
       "      <td>4.690847</td>\n",
       "      <td>5.758641</td>\n",
       "      <td>2.618974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.954786</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>0.203404</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>External security challenge was not satisfied.</td>\n",
       "      <td>1.612023</td>\n",
       "      <td>1.016405</td>\n",
       "      <td>19.431698</td>\n",
       "      <td>3.044661</td>\n",
       "      <td>z</td>\n",
       "      <td>attacktarget@domain.com</td>\n",
       "      <td>2022-08-31T23:54:50.435683000Z</td>\n",
       "      <td>DFP-azure-generic_user:543</td>\n",
       "      <td>2023-04-29T01:09:59Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  logcount  logcount_pred  logcount_loss  logcount_z_loss  \\\n",
       "0            22        22       6.294001      13.834214        10.950698   \n",
       "1            23        23       6.289782      15.659849        12.508736   \n",
       "2            24        24       6.287379      17.594990        14.160229   \n",
       "3            25        25       6.285878      19.640940        15.906289   \n",
       "4            26        26       6.283889      21.800470        17.749281   \n",
       "..          ...       ...            ...            ...              ...   \n",
       "432         152        16       7.445123       3.564877         1.963678   \n",
       "433         642        13       4.775130       3.045915         1.131403   \n",
       "434         643        14       4.683823       3.907826         1.604047   \n",
       "435         644        15       4.910330       4.583674         1.974660   \n",
       "436         646        16       4.690847       5.758641         2.618974   \n",
       "\n",
       "     locincrement  locincrement_pred  locincrement_loss  locincrement_z_loss  \\\n",
       "0             1.0           2.423363           4.199121             2.667636   \n",
       "1             1.0           2.424431           4.205431             2.672908   \n",
       "2             1.0           2.425592           4.212285             2.678633   \n",
       "3             1.0           2.426562           4.218020             2.683426   \n",
       "4             1.0           2.427629           4.224331             2.688698   \n",
       "..            ...                ...                ...                  ...   \n",
       "432           1.0           1.780383           4.898170             1.954723   \n",
       "433           1.0           1.954752           0.525846             0.203420   \n",
       "434           1.0           1.957831           0.529244             0.201969   \n",
       "435           1.0           1.949175           0.519721             0.206035   \n",
       "436           1.0           1.954786           0.525884             0.203404   \n",
       "\n",
       "     appincrement  ...                           statusfailureReason_pred  \\\n",
       "0             1.0  ...  Fresh auth token is needed. Have the user re-s...   \n",
       "1             1.0  ...  Fresh auth token is needed. Have the user re-s...   \n",
       "2             1.0  ...  Fresh auth token is needed. Have the user re-s...   \n",
       "3             1.0  ...  Fresh auth token is needed. Have the user re-s...   \n",
       "4             1.0  ...  Fresh auth token is needed. Have the user re-s...   \n",
       "..            ...  ...                                                ...   \n",
       "432          16.0  ...                                             _other   \n",
       "433          14.0  ...     External security challenge was not satisfied.   \n",
       "434          15.0  ...     External security challenge was not satisfied.   \n",
       "435          16.0  ...     External security challenge was not satisfied.   \n",
       "436          16.0  ...     External security challenge was not satisfied.   \n",
       "\n",
       "    statusfailureReason_loss  statusfailureReason_z_loss   max_abs_z  \\\n",
       "0                   1.926561                    1.058883   10.950698   \n",
       "1                   1.925017                    1.089649   12.508736   \n",
       "2                   1.923433                    1.121214   14.160229   \n",
       "3                   1.922159                    1.146584   15.906289   \n",
       "4                   1.921002                    1.169640   17.749281   \n",
       "..                       ...                         ...         ...   \n",
       "432                -0.000000                    0.000000  332.180115   \n",
       "433                 1.680271                    0.783752   13.603901   \n",
       "434                 1.630660                    0.952875   16.555244   \n",
       "435                 1.599877                    1.057811   19.275707   \n",
       "436                 1.612023                    1.016405   19.431698   \n",
       "\n",
       "     mean_abs_z  z_loss_scaler_type                 username  \\\n",
       "0      2.041574                   z         acole@domain.com   \n",
       "1      2.214617                   z         acole@domain.com   \n",
       "2      2.397326                   z         acole@domain.com   \n",
       "3      2.589084                   z         acole@domain.com   \n",
       "4      2.791408                   z         acole@domain.com   \n",
       "..          ...                 ...                      ...   \n",
       "432   40.882931                   z  attacktarget@domain.com   \n",
       "433    2.070845                   z  attacktarget@domain.com   \n",
       "434    2.588548                   z  attacktarget@domain.com   \n",
       "435    3.001436                   z  attacktarget@domain.com   \n",
       "436    3.044661                   z  attacktarget@domain.com   \n",
       "\n",
       "                          timestamp                        model_version  \\\n",
       "0    2022-08-30T01:54:26.639083000Z         DFP-azure-acole@domain.com:2   \n",
       "1    2022-08-30T01:54:26.639083000Z         DFP-azure-acole@domain.com:2   \n",
       "2    2022-08-30T01:54:26.639083000Z         DFP-azure-acole@domain.com:2   \n",
       "3    2022-08-30T01:54:26.639083000Z         DFP-azure-acole@domain.com:2   \n",
       "4    2022-08-30T01:54:26.639083000Z         DFP-azure-acole@domain.com:2   \n",
       "..                              ...                                  ...   \n",
       "432  2022-08-31T23:54:50.435683000Z  DFP-azure-attacktarget@domain.com:2   \n",
       "433  2022-08-31T23:45:08.870131000Z           DFP-azure-generic_user:543   \n",
       "434  2022-08-31T23:48:48.732047000Z           DFP-azure-generic_user:543   \n",
       "435  2022-08-31T23:51:17.852920000Z           DFP-azure-generic_user:543   \n",
       "436  2022-08-31T23:54:50.435683000Z           DFP-azure-generic_user:543   \n",
       "\n",
       "               event_time  \n",
       "0    2023-04-29T01:09:14Z  \n",
       "1    2023-04-29T01:09:14Z  \n",
       "2    2023-04-29T01:09:14Z  \n",
       "3    2023-04-29T01:09:14Z  \n",
       "4    2023-04-29T01:09:14Z  \n",
       "..                    ...  \n",
       "432  2023-04-29T01:09:59Z  \n",
       "433  2023-04-29T01:09:59Z  \n",
       "434  2023-04-29T01:09:59Z  \n",
       "435  2023-04-29T01:09:59Z  \n",
       "436  2023-04-29T01:09:59Z  \n",
       "\n",
       "[437 rows x 44 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cudf.read_csv(\"dfp_detections_azure.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
