<!--
SPDX-FileCopyrightText: Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Morpheus LLM Agents Example

The Morpheus LLM Agents example showcases the implementation of a pipeline that integrates LLM agents into the Morpheus
framework. This example demonstrates the efficient execution of multiple LLM queries using the ReAct agent type, which
is well-suited for handling a diverse range of tasks. The pipeline incorporates the use of the Langchain library to run
LLM agents, thereby streamlining the process and minimizing the need for additional system migration.

## Background on Agents

In the context of Morpheus LLM (Large Language Model) Agents, these agents act as intermediaries that facilitate communication between users and the LLM service. They enable the execution of tools and multiple LLM queries, thereby
enhancing the capabilities of the LLM in solving complex tasks. Agents make use of various tools such as internet
search, VDB retrievers, calculators, and more to aid in resolving complex inquiries. They can be integrated into the
Morpheus pipeline, allowing for seamless execution of tasks and efficient handling of queries.

## Getting Started

### Prerequisites

#### Install Dependencies

Install the required dependencies.

```bash
mamba env update -n morpheus --file ${MORPHEUS_ROOT}/docker/conda/environments/cuda11.8_examples.yml
```

## Simple Example

This example demonstrates the basic implementation of Morpheus pipeline, showcasing the process of executing LLM queries and managing the generated responses. It uses different stages such as InMemorySourceStage, DeserializationStage, ExtracterNode, LangChainAgentNode, SimpleTaskHandler, and InMemorySinkStage within the pipeline to handle various aspects of query processing and response management.

- Utilizes stages such as InMemorySourceStage and DeserializationStage for consuming and batching LLM queries.
- Incorporates an ExtracterNode for extracting questions and a LangChainAgentNode for executing the Langchain agent executor.
- SimpleTaskHandler to manage the responses generated by the LLMs.
- Stores and manages the results within the pipeline using an InMemorySinkStage.

### Running the Simple Example

```bash
cd ${MORPHEUS_ROOT}/llm

python main.py agents simple
```

## Kafka Example

The Kafka Example in the Morpheus LLM Agents demonstrates an streaming implementation, utilizing Kafka messages to
facilitate the near real-time processing of LLM queries. This example is similar to the Simple example but makes use of
a KafkaSourceStage to stream and retrieve messages from the Kafka topic

### Running the Kafka Example

First, to run the Kafka example, you need to create a Kafka cluster that enables the persistent pipeline to accept queries for the LLM agents. You can create the Kafka cluster using the following guide: [Quick Launch Kafka Cluster Guide](https://github.com/nv-morpheus/Morpheus/blob/branch-23.11/docs/source/developer_guide/contributing.md#quick-launch-kafka-cluster)

Once the Kafka cluster is running, the Kafka example can be run using the following command:

```bash
cd ${MORPHEUS_ROOT}/llm

python main.py agents kafka
```

After the pipeline is running, we need to send messages to the pipeline using the Kafka topic. In a separate terminal, run the following command:

```bash
# Set the bootstrap server variable
export BOOTSTRAP_SERVER=$(broker-list.sh)

# Create the input and output topics
kafka-topics.sh --bootstrap-server ${BOOTSTRAP_SERVER} --create --topic input

# Update the partitions
kafka-topics.sh --bootstrap-server ${BOOTSTRAP_SERVER} --alter --topic input --partitions 3
```

Now, we can send messages to the pipeline using the following command:

```bash
kafka-console-producer.sh --bootstrap-server ${BOOTSTRAP_SERVER} --topic input
```

This will open up a prompt allowing any JSON to be pasted into the terminal. The JSON should be formatted as follows:

```json
{"question": "<Your question here>"}
```

For example:
```json
{"question": "Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?"}
{"question": "What is the height of the tallest mountain in feet divided by 2.23? Do not round your answer"}
{"question": "Who is the current leader of Japan? What is the largest prime number that is smaller that their age? Just say the number."}
```
