<!--
SPDX-FileCopyrightText: Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# 5. Digital Fingerprinting (DFP)

## Overview
Every account, user, service and machine has a digital fingerprint​, which represents the typical actions performed and not performed over a given period of time.  Understanding every entity's day-to-day, moment-by-moment work helps us identify anomalous behavior and uncover potential threats in the environment​.

To construct this digital fingerprint we will be training unsupervised behavioral models at various granularities, including a generic model for all users in the organization along with fine-grained models for each user to monitor their behavior. These models are continuously updated and retrained overtime​, and alerts are triggered when deviations from normality occur for any user​.

## Training Sources
The data we will want to use for the training and inference will be any sensitive system that the user interacts with, such as VPN, authentication and cloud services. The [digital fingerprinting example](/examples/digital_fingerprinting/README.md) included in Morpheus ingests logs from [AWS CloudTrail](https://docs.aws.amazon.com/cloudtrail/index.html), [Azure Active Directory](https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-sign-ins) and [Duo Authentication](https://help.duo.com/s/article/1023?language=en_US).

The location of these logs could be either local to the machine running Morpheus, a shared filesystem like NFS or on a remote store such as [Amazon S3](https://aws.amazon.com/s3/).

Additional data sources and remote stores can easily be added using the Morpheus SDK, the key to applying DFP to a new data source is through the process of feature selection. Any data source can be fed into DFP after some preprocessing to get a feature vector per log/data point​.  Since DFP builds a targeted model for each entity (user/service/machine... etc.), it would work best if the chosen data source has a field that uniquely identifies the entity we’re trying to model.


### DFP Features

#### AWS CloudTrail
| Feature | Description |
| ------- | ----------- |
| userIdentityaccessKeyId | e.g., ACPOSBUM5JG5BOW7B2TR, ABTHWOIIC0L5POZJM2FF, AYI2CM8JC3NCFM4VMMB4 |
| userAgent | e.g., Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 10.0; Trident/5.1), Mozilla/5.0 (Linux; Android 4.3.1) AppleWebKit/536.1 (KHTML, like Gecko) Chrome/62.0.822.0 Safari/536.1, Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10 7_0; rv:1.9.4.20) Gecko/2012-06-10 12:09:43 Firefox/3.8 |
| userIdentitysessionContextsessionIssueruserName | e.g., role-g |
| sourceIPAddress | e.g., 208.49.113.40, 123.79.131.26, 128.170.173.123 |
| userIdentityaccountId | e.g., Account-123456789 |
| errorMessage | e.g., The input fails to satisfy the constraints specified by an AWS service., The specified subnet cannot be found in the VPN with which the Client VPN endpoint is associated., Your account is currently blocked. Contact aws-verification@amazon.com if you have questions. |
| userIdentitytype | e.g., FederatedUser |
| eventName | e.g., GetSendQuota, ListTagsForResource, DescribeManagedPrefixLists |
| userIdentityprincipalId | e.g., 39c71b3a-ad54-4c28-916b-3da010b92564, 0baf594e-28c1-46cf-b261-f60b4c4790d1, 7f8a985f-df3b-4c5c-92c0-e8bffd68abbf |
| errorCode | e.g., success, MissingAction, ValidationError |
| eventSource | e.g., lopez-byrd.info, robinson.com, lin.com |
| userIdentityarn | e.g., arn:aws:4a40df8e-c56a-4e6c-acff-f24eebbc4512, arn:aws:573fd2d9-4345-487a-9673-87de888e4e10, arn:aws:c8c23266-13bb-4d89-bce9-a6eef8989214 |
| apiVersion | e.g., 1984-11-26, 1990-05-27, 2001-06-09 |

#### Azure Active Directory
| Feature | Description |
| ------- | ----------- |
| appDisplayName | e.g., Windows sign in, MS Teams, Office 365​ |
| clientAppUsed | e.g., IMAP4, Browser​ |
| deviceDetail.displayName | e.g., username-LT​ |
| deviceDetail.browser | e.g., EDGE 98.0.xyz, Chrome 98.0.xyz​ |
| deviceDetail.operatingSystem | e.g., Linux, IOS 15, Windows 10​ |
| statusfailureReason | e.g., external security challenge not satisfied, error validating credentials​ |
| riskEventTypesv2 | AzureADThreatIntel, unfamiliarFeatures​ |
| location.countryOrRegion | country or region name​ |
| location.city | city name |

##### Derived Features
| Feature | Description |
| ------- | ----------- |
| logcount | tracks the number of logs generated by a user within that day (increments with every log)​ |
| locincrement | increments every time we observe a new city (location.city) in a user’s logs within that day​ |
| appincrement | increments every time we observe a new app (appDisplayName) in a user’s logs within that day​ |

#### Duo Authentication
| Feature | Description |
| ------- | ----------- |
| auth_device.name | phone number​ |
| access_device.browser | e.g., Edge, Chrome, Chrome Mobile​ |
| access_device.os | e.g., Android, Windows​ |
| result | SUCCESS or FAILURE ​ |
| reason | reason for the results, e.g., User Cancelled, User Approved, User Mistake, No Response​ |
| access_device.location.city | city name |

##### Derived Features
| Feature | Description |
| ------- | ----------- |
| logcount| tracks the number of logs generated by a user within that day (increments with every log)​ |
| locincrement | increments every time we observe a new city (location.city) in a user’s logs within that day​ |


## High Level Architecture
DFP in Morpheus is accomplished via two independent pipelines: training and inference​. The pipelines communicate via a shared model store ([MLflow](https://mlflow.org/)), and both share many common components​, as Morpheus is composed of reusable stages which can be easily mixed and matched.

![High Level Architecture](img/dfp_high_level_arch.png)

#### Training Pipeline
* Trains user models and uploads to the model store​
* Capable of training individual user models or a fallback generic model for all users​

#### Inference Pipeline
* Downloads user models from the model store​
* Generates anomaly scores per log​
* Sends detected anomalies to monitoring services

#### Monitoring
* Detected anomilies are published to an S3 bucket, directory or a Kafka topic.
* Output can be integrated with a monitoring tool.


## Runtune Environment Setup
![Runtune Environment Setup](img/dfp_runtime_env.png)

DFP in Morpheus is built as an application of containerized services​ and can be run in two ways:
1. Using docker-compose for testing and development​
1. Using helm charts for production Kubernetes deployment​

### System requirements for the DFP reference architecture:
##### Running via docker-compose:
* [Docker](https://docs.docker.com/get-docker/) and [docker-compose](https://docs.docker.com/compose/) installed on the host machine​
* Supported GPU with [nvidia-docker runtime​]((https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker))

##### Running via Kubernetes​
* [Kubernetes](https://kubernetes.io/) cluster configured with GPU resources​
* [NVIDIA GPU Operator](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/gpu-operator) installed in the cluster

Note: For GPU Requirements see [README.md](/README.md#requirements)

### Services
The reference architecture is composed of the following services:​
| Service | Description |
| ------- | ----------- |
| [MLflow](https://mlflow.org/) | Provides a versioned model store​ |
| [Jupyter Server](https://jupyter-notebook.readthedocs.io/en/stable/public_server.html)​ | Necessary for testing and development of the pipelines​ |
| Morpheus Training Pipeline​ | Trains the autoencoder models and uploads to MLFlow |
| Morpheus Inference Pipeline​ | Downloads models from MLFlow for inferencing​ & Publishes anomalies |


## Morpheus Configuration
![Morpheus Configuration](img/dfp_deployment_configs.png)

### Pipeline Structure Configuration
![Pipeline Structure Configuration](img/dfp_pipeline_structure.png)

The stages in both the Training and Inference pipelines can be mixed and matched with little impact​, i.e., the `MultiFileSource` can be configured to pull from S3 or from local files and can be replaced altogether with any other Morpheus input stage. similarly the S3 writer can be replaced with any Morpheus output stage.  Regardless of the inputs & outputs the core pipeline should renmain unchanged.  While stages in the core of the pipeline (inside the blue areas in the above diagram) perform common actions that should be configured not exchanged.

### Morpheus Config

For both inference and training pipeline the Morpheus config object should be constructed with the same values, and should look like:
```python
import os

from morpheus.config import Config
from morpheus.config import ConfigAutoEncoder
from morpheus.config import CppConfig
from morpheus.cli.utils import get_package_relative_file
from morpheus.cli.utils import load_labels_file
```
```python
CppConfig.set_should_use_cpp(False)

config = Config()
config.num_threads = os.cpu_count()
config.ae = ConfigAutoEncoder()
config.ae.feature_columns = load_labels_file(get_package_relative_file("data/columns_ae_azure.txt"))
```

Other attributes which might be needed:
| Attribute | Type | Default | Description |
| --------- | ---- | ------- | ----------- |
| `Config.ae.userid_column_name` | `str` | `userIdentityaccountId` | Column in the `DataFrame` containing the username or user ID |
| `Config.ae.timestamp_column_name` |  `str` | `timestamp` | Column in the `DataFrame` containing the timestamp of the event |
| `Config.ae.fallback_username` | `str` | `generic_user` | Name to use for the generic user model, shoudl nto match the name of any real users |

### Input Stages
![Input Stages](img/dfp_input_config.png)

#### MultiFileSource
The `MultiFileSource`([`examples/digital_fingerprinting/production/morpheus/dfp/stages/multi_file_source.py`](/examples/digital_fingerprinting/production/morpheus/dfp/stages/multi_file_source.py)) receives a path or list of paths (`filenames`), and will collectively be emitted into the pipeline as an [fsspec.core.OpenFiles](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.core.OpenFiles) object.  The paths may include wildcards `*` as well as urls (ex: `s3://path`) to remote storage providers such as S3, FTP, GCP, Azure, Databricks and others as defined by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/api.html?highlight=open_files#fsspec.open_files).  In addition to this paths can be cached locally by prefixing them with `filecache::` (ex: `filecache::s3://bucket-name/key-name`).

Note: this stage does not actually download the data files, allowing the file list to be filtered and batched prior to being downloaded.

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `filenames` | `List[str]` or `str` | Paths to source file to be read from |


#### DFPFileBatcherStage
The `DFPFileBatcherStage` ([`examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_file_batcher_stage.py`](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_file_batcher_stage.py)) groups data in the incoming `DataFrame` in batches of a time period (per day default).  This stage assumes that the date of the logs in S3 can be easily inferred such as encoding the creation time in the file name (e.g., `AUTH_LOG-2022-08-21T22.05.23Z.json`), the actual method for extracting the date is encoded in a user-supplied `date_conversion_func` function (more on this later).

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `date_conversion_func` | `function` | Function receives a single [fsspec.core.OpenFile](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.core.OpenFile) argument and returns a `datetime.datetime` object |
| `period` | `str` | Time period to group data by, value must be [one of pandas' offset strings](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases) |
| `sampling_rate_s` | `int` | Optional, default=`0`. When non-zero a subset of the incoming data files will be sampled, only including a file if the `datetime` returned by `date_conversion_func` is at least `sampling_rate_s` seconds greater than  the `datetime` of the previously included file |

For situations where the creation date of of the log file is encoded in the filename is desired, the `date_extractor` in the [`examples/digital_fingerprinting/production/morpheus/dfp/utils/file_utils.py`](/examples/digital_fingerprinting/production/morpheus/dfp/utils/file_utils.py) module can be used.  The `date_extractor` will need to have a regex pattern bound to it before being passed in as a parameter to `DFPFileBatcherStage`. The regex pattern will need to contain the following named groups: `year`, `month`, `day`, `hour`, `minute`, `second`, and optionally `microsecond`.

For input files containing an ISO 8601 formatted date string the `iso_date_regex` regex can be used ex:
```python
from functools import partial

from dfp.utils.file_utils import date_extractor
from dfp.utils.file_utils import iso_date_regex
```
```python
# Batch files into buckets by time. Use the default ISO date extractor from the filename
pipeline.add_stage(
    DFPFileBatcherStage(config,
                        period="D",
                        date_conversion_func=functools.partial(date_extractor, filename_regex=iso_date_regex)))
```

Note: in cases where the regular expression does not match the `date_extractor` function will fallback to using the modified time of the file.

#### DFPFileToDataFrameStage
The `DFPFileToDataFrameStage` ([examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_file_to_df.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_file_to_df.py)) stage receives a `list` of an [fsspec.core.OpenFiles](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.core.OpenFiles) and loads them into a single `DataFrame` which is then emitted into the pipeline.  When the parent stage is `DFPFileBatcherStage` each batch (typically one day) is concatenated into a single `DataFrame`, without this if the parent was `MultiFileSource` the entire dataset is loaded into a single `DataFrame`.  Because of this, it is important to chose a `period` argument for `DFPFileBatcherStage` small enough such that each batch can fit into memory.

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `schema` | `dfp.utils.column_info.DataFrameInputSchema` | Schema specifying columns to load, along with any necessary renames and data type conversions  |
| `filter_null` | `bool` | Optional: Whether to filter null rows after loading, by default True. |
| `file_type` | `morpheus._lib.file_types.FileTypes` (enum) | Optional: Indicates file type to be loaded. Currently supported values at time of writing are: `FileTypes.Auto`, `FileTypes.CSV`, and `FileTypes.JSON`. Default value is `FileTypes.Auto` which will infer the type based on the file extension, set this value if using a custom extension |
| `parser_kwargs` | `dict` or `None` | Optional: additional keyword arguments to be passed into the `DataFrame` parser, currently this is going to be either [`pandas.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) or [`pandas.read_json`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) |
| `cache_dir` | `str` | Optional: path to cache location, defaults to `./.cache/dfp` |

This stage is able to download & load data files concurrently by multiple methods, currently supported methods are: `single_thread`, `multiprocess`, `dask`, and `dask_thread`.  The method used is chosen by setting the `FILE_DOWNLOAD_TYPE` environment variable, and `dask_thread` is used by default, and `single_thread` effectively disables concurrent loading.

This stage will cache the resulting `DataFrame` in `cache_dir`, since we are caching the `DataFrame`s and not the source files, a cache hit avoids the cost of parsing the incoming data. In the case of remote storage systems such as S3 this avoids both parsing and a download on a cache hit.  One consequence of this is that any change to the `schema` will require purging cached files in the `cache_dir` before those changes are visible.

Note: this caching is in addition to any caching which may have occurred when using the optional `filecache::` prefix.

### DataFrameInputSchema
TODO: Document input schemas and `ColumnInfo` subclasses

### Output Stages
![Output Stages](img/dfp_output_config.png)

For the inference pipeline any Morpheus output stage such as `morpheus.stages.output.write_to_file_stage.WriteToFileStage` and `morpheus.stages.output.write_to_kafka_stage.WriteToKafkaStage` could be used in addition to the `WriteToS3Stage` documented below.

#### WriteToS3Stage
The `WriteToS3Stage` ([examples/digital_fingerprinting/production/morpheus/dfp/stages/write_to_s3_stage.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/write_to_s3_stage.py)) stage writes the resulting anomaly detections to S3.  The `WriteToS3Stage` decouples the S3 specifc operations from the Morpheus stage, and as such receives an `s3_writer` argument.

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `s3_writer` | `function` | User defined function which receives an instance of a `morpheus.messages.message_meta.MessageMeta` and returns that same message instance. Any S3 specific configurations such as bucket name should be bound to the method. |

### Core Pipeline
These stages are common to both the training & inference pipelines, unlike the input & output stages these are specific to the DFP pipeline and intended to be configured but not replacable.

#### DFPSplitUsersStage
TODO

#### DFPRollingWindowStage
TODO

#### DFPPreprocessingStage
TODO


## Training Pipeline
![Training PipelineOverview](img/dfp_training_overview.png)

Training must begin with the generic user model​ which is trained with the logs from all users.  This model serves as a fallback model for users & accounts without sufficient training data​.  The name of the generic user is defined in the `ae.fallback_username` attribute of the Morpheus config object and defaults to `generic_user`.

After training the generic model, individual user models can be trained​.  Individual user models provide better accuracy but require sufficient data​, many users do not have sufficient data to accurately train the model​.

### Training Stages

#### DFPTraining
The `DFPTraining` [examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_training.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_training.py) trains a model for each incoming `DataFrame` and emits an instance of `morpheus.messages.multi_ae_message.MultiAEMessage` containing the trained model.

#### DFPMLFlowModelWriterStage
The `DFPMLFlowModelWriterStage` [examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_mlflow_model_writer.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_mlflow_model_writer.py) stage publishes trained models into MLflow, skipping any model which lacked sufficient training data (current required minimum is 300 log records).

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `model_name_formatter` | `str` | Optional format string to control the name of models stored in MLflow, default is `dfp-{user_id}`. Currently available field names are: `user_id` and `user_md5` which is an md5 hexadecimal digest as returned by [`hash.hexdigest`](https://docs.python.org/3.8/library/hashlib.html?highlight=hexdigest#hashlib.hash.hexdigest). |
| `experiment_name_formatter` | `str` | Optional format string to control the experiement name for models stored in MLfklow, default is `/dfp-models/{reg_model_name}`.  Currently available field names are: `user_id`, `user_md5` and `reg_model_name` which is the model name as defined by `model_name_formatter` once the field names have been applied. |
| `databricks_permissions` | `dict` or `None` | Optional, when not `None` sets permissions needed when using a databricks hosted MLflow server |

Note: If using a remote MLflow server, users will need to call [`mlflow.set_tracking_uri`](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tracking_uri) before starting the pipeline.

## Inference Pipeline
![Inference Pipeline Overview](img/dfp_inference_overview.png)

### Inference Stages

#### DFPInferenceStage
The `DFPInferenceStage` [examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_inference_stage.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_inference_stage.py) stage loads models from MLflow and performs inferences against those models.  This stage emits a message containing the original `DataFrame` along with new columns containing the anomaly score (`mean_abs_z`), along with the name and version of the model that generated that score (`model_version`).  For each feature in the model three additional columns will also be added:
* `<feature name>_loss` : The loss
* `<feature name>_z_loss` : The loss z-score
* `<feature name>_pred` : The predicted value

For a hypothtetical feature named `result` the three added columns will be: `result_loss`, `result_z_loss`, `result_pred`.

For performance models fetched from MLflow are cached locally, and are cached for up to 10 minutes allowing updated models to be routinely updated.  In addition to caching individual models the stage also maintains a cache of which models are available, so a newly trained user model published to MLflow won't be visible to an already running inference pipeline for up to 10 minutes.

For any user without an associated model in MLflow, the model for the generic user is used. The name of the generic user is defined in the `ae.fallback_username` attribute of the Morpheus config object defaults to `generic_user`.

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `model_name_formatter` | `str` | Format string to control the name of models fetched from MLflow.  Currently available field names are: `user_id`. |


#### DFPPostprocessingStage
The `DFPPostprocessingStage` [examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_postprocessing_stage.py](/examples/digital_fingerprinting/production/morpheus/dfp/stages/dfp_postprocessing_stage.py) stage

| Argument | Type | Descirption |
| -------- | ---- | ----------- |
| `c` | `morpheus.config.Config` | Morpheus config object |
| `z_score_threshold` | `float` | Optional, sets the threshold value above which values of `mean_abs_z` must be above in order to be considered  an anomily, default is 2.0 |
